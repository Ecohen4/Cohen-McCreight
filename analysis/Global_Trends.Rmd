---
title: 'Global Trends In Urban Energy Use: The Tropical Shift'
author: "Elliot Cohen, Vijay Modi, Henri Torbey, Yu Tian and Michael Piccirilli"
date: "October 9 2014"
output:
  pdf_document: default
  html_document:
    theme: spacelab
---

*************
```{r global_options, include=FALSE}
library(knitr)
opts_chunk$set(fig.path='Figs/', echo=FALSE, fig.width=8, fig.height=5, cache=TRUE)
options(width=85)
```

```{r initialize, , include=TRUE, message=FALSE}
setwd("~/Google Drive/Global_Trends/Data")
filepath<-"~/github/Rcommands/" ## This was the original file path, i will modify slightly


## read-in csv files from NRLDC html data scapring (Chris Tan and Myf March 2014)
options(stringsAsFactors=FALSE)

## Load libraries
# library(knitr)    # create RMD documents
# library(rmarkdown)
# library(plyr)     # ddply...
# library(reshape2) # melt...
# library(ggplot2)  # plots
# library(scales)   # used in ggplot()
# library(xlsx)     # read excel files
# library(sm)       # density plots
# library(ggmap)    # geocode


## The following function will load the packages required for this tutorial.  If a package cannot be found in your instance of Rstudio, it will automatically be insalled.
## require() returns (invisibly) a logical indicating whether the required package is available.
load_install<-function(lib){
  if(! require(lib, character.only=TRUE)) install.packages(lib, character.only=TRUE) 
  library(lib, character.only=TRUE, quietly=TRUE)
}

## the required libraries (e.g. packages)
Thelib<-c("knitr", "rmarkdown", "plyr", "reshape2", "ggplot2", "scales", "xlsx", "sm", "ggmap")

## apply the function
lapply(Thelib, load_install)

## load data
load("growth.rsav")         # 5-yr average population growth in cities w. pop > 750k (as reported)
load("growth.melt.rsav")    # "" melted
load("pop.rsav")            # 5-yr average population in cities w. pop > 750k (as reported)
load("pop.melt.rsav")       # "" melted
load("growth.decadal.rsav")  # 10-yr population growth (computed for '50-60, ..., '10-20)
load("growth.decadal2.rsav") # 10-yr population growth (computed for '55-65, ..., '15-25)
load("cities.rsav") # synthesized population and growth rate data for global cities with pop > 750k
load("epochal.rsav") # "cities" summarized by 25-yr epoch
```

```{r functions}
## function to create Figure/Table numbers
incCount <- function(inObj, useName) {
    nObj <- length(inObj)
    useNum <- max(inObj) + 1
    inObj <- c(inObj, useNum)
    names(inObj)[nObj + 1] <- useName
    inObj
}
figCount <- c(`_` = 0)
tableCount <- c(`_` = 0)
## Use like this:
# figCounts <- incCount(figCount, "f.randomFigure")

# function to check for NAs and complete cases
check<-function(df){
  NAs<-sum(is.na(df))
  print(paste("NAs:", NAs)) # count NA's
  if (NAs>0) df1[which(is.na(df)), ] # Show NA's, if any.
  cc<-complete.cases(df)  # check for missing values...
  print(paste("Complete Cases:", all(cc)))  # Given a set of logical vectors, are all of the values true?
  }
```

Motivation
-----------
Many of the world's **largest** and **fastest-growing** cities--from Karachi (pop. 14 million; 34.6% increase from 2000-2010) to Delhi (22m; 39.4%), Dhaka (15m, 45.2%), Jakarta (10m; 14.8%), Bangkok (8m, 29.1%), Lagos (11m; 48.2%) and Kinshasa (9m, 55.4%)--are located in South Asia and Sub-Saharan Africa with tropical to sub-tropical climates unlike those of most OECD member cities in the global North. As the tropics/sub-tropics become increasingly urban, inustrial and affluent, it is important to consider how energy demand--particularly for thermal comfort--will evolve differently in these places than it has historically across the OECD. 

To illustrate the potential for vast differences in energy demand for thermal comfort between cities in the global North and cities in the Tropics/Sub-Tropics, consider Delhi, India.  Delhi--with its massive population and very hot climate--is an outlier compared to OECD member cities but typical of South Asia: Peak summer temperatures routinely exceed 40 deg C. (104 F.), and intense heatwaves can approach 50 deg C. (122 F.). Given the huge temperature differential between outdoor (say 104 F.) and desired indoor air temperature (say 72 F), and the thermodynamic fact that energy for cooling scales linearly with the temperature differential, cooling a room in Delhi requires twice as much energy as cooling a room in New York where the summer outdoor-indoor temperature differential is typically half that. 

In addition to higher temperatures, the summer season is also much longer: in the past year, Delhi had over **six** times as many cooling-degree days as New York City (again assuming a desired indoor air temperature of 72 deg F). Compounded by leaky building envelopes in developing world cities (designed for natural ventilation, not air conditiong), intense heat-island effects (typically less green space), and massive population growth, peak electricity demand in cities thourought the developing world could one day surpass that of their neighbors to the north--not only in aggregate terms because of their population, but also *per-capita* due to climate, building design and thermodynamics. 

Implications of Global Energy Service Provision Parity
--------------
If we consider the trajectory of developing world cities as reaching eventual parity with OECD cities, and if we think in terms of energy *service provision* rather than just BTU or KWh, then we begin to appreciate the magnitude of future energy demand (and associated resource consumption and environmental impact). This has important implications not only for regional grid planning and supply reliability, but also the global transition to renewable energy given the limitations of meeting such large and 'peaky' demand with non-dispatchable resources such as wind and solar.

Figure 1 provides a map of urbanization rates for cities worldwide with a population greater than 750,000 (UN 2011). Urbanization rates are clearly highest in South Asia and Sub-Saharan Africa. Figure 2 shows the *distribution* of population growth rates for the same set of cities, grouped by latitude (global North, Tropics and Subtropics). The expected value (e.g. central tendancy) of urbanization rates in the Tropics and Sub-Tropics is clearly distinct from that of the North.

```{r urbanization_data, cache=FALSE, results='hide', eval=FALSE}
# ## Import data
# ## United Nations, Department of Economic and Social Affairs, Population Division
# ## World Urbanization Prospects: The 2011 Revision
# 
# # Average Annual Rate of Change of Urban Agglomerations with 750,000 Inhabitants or More in 2011, by Country, 1950-2025 (per cent)
# growth=read.xlsx(file="UN_2011_Growth_Rate_Cities_Over_750k.xlsx",
#                  sheetName="GROWTH-RATE-CITIES",
#                  as.data.frame=TRUE,header=TRUE,check.names=TRUE,
#                  startRow=13)
# 
# # Population of urban agglomerations with 750,00 inhabitants or more, 1950-2025
# pop=read.xlsx(file="UN_2011_Population_Cities_Over_750k.xlsx",
#               sheetName="CITIES-OVER-750K",
#               as.data.frame=TRUE,header=TRUE,check.names=TRUE,
#               startRow=13, endRow=646, colIndex=c(1:23))
# 
# # match city/country to continent
# countries<-scan(file="countrylist.txt",
#                 what="charachter", sep="\n",
#                           blank.lines.skip=TRUE)
# 
# dummy<-strsplit(countries, split=" ")
# continent<-laply(dummy, '[[', 1)
# country.code<-laply(dummy, '[[', 4)
# country.name<-laply(dummy, '[[', 5)
# df<-data.frame(Country.Code=country.code, Country.Name=country.name, Continent.Abr=continent)
# df$Country.Code<-as.numeric(df$Country.Code)
# 
# df$Continent<-NA
# df$Continent[which(df$Continent.Abr=="AF")]<-"Africa"
# df$Continent[which(df$Continent.Abr=="AS")]<-"Asia"
# df$Continent[which(df$Continent.Abr=="EU")]<-"Europe"
# df$Continent[which(df$Continent.Abr=="NA")]<-"North.America"
# df$Continent[which(df$Continent.Abr=="SA")]<-"South.America"
# df$Continent[which(df$Continent.Abr=="OC")]<-"Oceania"
# df$Continent[which(df$Continent.Abr=="AN")]<-"Antarctica"
# 
# # merge operation
# merge.by<-"Country.Code"
# print(paste("Both data.frames share a unqiue column name:", merge.by %in% names(pop) & merge.by %in% names(df), sep=" "))
# test<-merge(pop, df, by=merge.by)
# test<-subset(test, select=-Continent.Abr)
# test<-subset(test, select=-Country.Name)
# test<-subset(test, select=-Note)
# test<-subset(test, select=c(23, 1:22))
# pop<-test
# remove(test)
# save(pop, file="pop.rsav")
# 
# merge.by<-"Country.Code"
# print(paste("Both data.frames share a unqiue column name:", merge.by %in% names(growth) & merge.by %in% names(df), sep=" "))
# test<-merge(growth, df, by=merge.by)
# test<-subset(test, select=-Continent.Abr)
# test<-subset(test, select=-Country.Name)
# test<-subset(test, select=-Note)
# test<-subset(test, select=c(22, 1:21))
# growth<-test
# remove(test)
# save(growth, file="growth.rsav")
# 
# ## get data for a couple of well known megacities...
# get<-c("Karachi", "Delhi", "Dhaka", "Jakarta", "Krung Thep (Bangkok)", "Lagos", "Kinshasa")
# 
# ## compute decadal growth rates
# # even decades
# test<-pop[, 1:7]
# test$g50.60<-(pop$X1960-pop$X1950)/pop$X1950*100
# test$g60.70<-(pop$X1970-pop$X1960)/pop$X1960*100
# test$g70.80<-(pop$X1980-pop$X1970)/pop$X1970*100
# test$g80.90<-(pop$X1990-pop$X1980)/pop$X1980*100
# test$g90.00<-(pop$X2000-pop$X1990)/pop$X1990*100
# test$g00.10<-(pop$X2010-pop$X2000)/pop$X2000*100
# test$g10.20<-(pop$X2020-pop$X2010)/pop$X2010*100
# test[test$Urban.Agglomeration %in% get, ]
# growth.decadal<-test
# remove(test)
# save(growth.decadal, file="growth.decadal.rsav")
# 
# # odd decades
# test<-pop[, 1:7]
# test$g55.65<-(pop$X1965-pop$X1955)/pop$X1955*100
# test$g65.75<-(pop$X1975-pop$X1965)/pop$X1965*100
# test$g75.85<-(pop$X1985-pop$X1975)/pop$X1975*100
# test$g85.95<-(pop$X1995-pop$X1985)/pop$X1985*100
# test$g95.05<-(pop$X2005-pop$X1995)/pop$X1995*100
# test$g05.15<-(pop$X2015-pop$X2005)/pop$X2005*100
# test$g15.25<-(pop$X2025-pop$X2015)/pop$X2015*100
# test[test$Urban.Agglomeration %in% get, ]
# growth.decadal2<-test
# remove(test)
# save(growth.decadal2, file="growth.decadal2.rsav")
#
## START HERE if loading previously saved dataframes....
# load("pop.rsav")
# load("growth.rsav")
#
# ## melt for ggplot()
# df1<-melt(pop, id.vars=c(1:7))
# sum(is.na(df1))
# 
# ## assign pretty column names
# get<-levels(df1$variable)
# dummy<-strsplit(get, split="X")
# test<-laply(dummy, '[[', 2)
# df1$variable<-factor(df1$variable, labels=test)
# 
# ## check for NA's
# sum(is.na(df1))          # zero NAs
# df1[which(is.na(df1)), ] # zero NAs
# cc<-complete.cases(df1)  # check for missing values...
# sum(cc)==dim(df1)[1]     # zero missing values.
# 
# ## bin growth rates by quantile
# df1$bin<-cut(df1$value, breaks=quantile(df1$value), include.lowest=TRUE)
# df1$bin.name<-factor(df1$bin, labels=c("1st quartile","2nd quartile","3rd quartile","4th quartile")) # assign pretty names
# names(df1)[8:11]<-c("Year","Pop.","Pop.Bin","Pop.Quartile")
# pop.melt<-df1
# 
# ## check for NAs and complete cases
# NAs<-sum(is.na(df1)); NAs           # count NA's
# if (NAs>0) df1[which(is.na(df1)), ] # Show NA's, if any.
# cc<-complete.cases(df1) # check for missing values...
# all(cc)                 # Given a set of logical vectors, are all of the values true?
# 
# ## repeat for growth data
# df2<-melt(growth, id.vars=c(1:7))
# get<-levels(df2$variable)
# dummy<-strsplit(get, split="X")
# period<-laply(dummy, '[[', 2)
# period<-gsub("[.]", "-", period)
# 
# ## bin growth rates by quantile
# df2$variable<-factor(df2$variable, labels=period)
# df2$bin<-cut(df2$value, breaks=quantile(df2$value), include.lowest=TRUE)
# df2$bin.name<-factor(df2$bin, labels=c("1st quartile","2nd quartile","3rd quartile","4th quartile"))
# names(df2)[8:11]<-c("Period","Growth","Growth.Bin","Growth.Quartile")
# growth.melt<-df2
# 
# ## save
# save(pop.melt, file="pop.melt.rsav")
# save(growth.melt, file="growth.melt.rsav")
# 
# # conform population data and growth rate data for a given year
# dummy<-strsplit(as.character(df2$Period), split="-")  # year ending XXXX
# df2$Year<-laply(dummy, "[[", 2)
# str(df2)
# 
# # check for NAs and complete cases
# NAs<-sum(is.na(df2)); NAs           # count NA's
# if (NAs>0) df1[which(is.na(df2)), ] # Show NA's, if any.
# cc<-complete.cases(df2) # check for missing values...
# all(cc)                 # Given a set of logical vectors, are all of the values true?
# 
# # MERGE
# merge.by<-c("Continent", "Country.Code", "Country", "City.Code", "Urban.Agglomeration", "Latitude", "Longitude", "Year")
# print(paste("Both data.frames share a unqiue column name:", merge.by %in% names(df1) & merge.by %in% names(df2), sep=" "))
# 
# cities<-merge(df1, df2, by=merge.by)
# 
# # apply the function
# check(cities)
#
# ## group by geography... (or try clustering by lat-long attributes)
# tropics<-subset(cities, Latitude>=-23.45 & Latitude <=23.45)
# subtropics<-subset(cities, Latitude>=-38 & Latitude <=-23.45 | Latitude>=23.45 & Latitude <=38)
# north<-subset(cities, Latitude>38)
# 
# ## add region attribute to cities
# cities$region<-NA
# t<-which(cities$Latitude >= -23.45 & cities$Latitude <= 23.45)
# cities$region[t]<-"Tropics"
# 
# st<-which(cities$Latitude>= -38 & cities$Latitude < -23.45 | cities$Latitude > 23.45 & cities$Latitude <= 38)
# cities$region[st]<-"Subtropics"
# 
# n<-which(cities$Latitude > 38)
# cities$region[n]<-"North"
# cities$region<-as.factor(cities$region)
# 
# # ## group countries by UN economic classifications
# # D1<-c("Andorra", "Australia", "Bermuda", "Canada", "Iceland", "Israel", "Liechtenstein", "Monaco", "New Zealand", "Norway", "Korea (South)", "San Marino", "Singapore", "Switzerland", "Taiwan")
# # 
# # D2:  "Albania", "Algeria", "Anguilla", "Antigua and Barbuda", "Argentina", "Aruba", "Bahamas", "Bahrain", "Barbados", "Belize", "Bolivia", "Bosnia and Herzegovina", "Botswana", "Brazil", "British Virgin Islands", "Brunei Darussalam", "Bulgaria", "Cameroon", "Cayman Islands", "Chile", "Colombia", "Cook Islands", "Costa Rica", "Cote d'Ivoire", "Croatia", "Cuba"," Czechoslovakia (Former)", "Korea (North)", "Dominica", "Dominican Republic", "Ecuador", "Egypt", "El Salvador", "Fiji", "French Polynesia", "Gabon", "Ghana", "Grenada", "Guatemala", "Haiti", "Honduras", "Indonesia", "Iran", "Iraq", "Jamaica", "Jordan", "Kenya", "Kuwait", "Lebanon", "Libyan Arab Jamahiriya", "Malaysia", Marshall Islands, Mauritius, Mexico, Micronesia, Mongolia, Montserrat , Morocco, Namibia, Nauru, Netherlands Antilles, New Caledonia, Nicaragua, Nigeria, Occupied Palestine, Oman, Pakistan, Palau, Panama, Papua New Guinea, Paraguay, Peru, Philippines, Puerto Rico, Qatar, Romania, Saint Kitts-Nevis, Saint Lucia, Saint Vincent and the Grenadines, Saudi Arabia, Serbia and Montenegro, Seychelles, Somalia, South Africa, Sri Lanka, Sudan, Suriname, Swaziland, Syria, Macedonia, Thailand, Tonga, Trinidad and Tobago, Tunisia, Turkey, Turks and Caicos Islands, United Arab Emirates, Uruguay, Venezuela, Vietnam, Zanzibar, Zimbabwe
# # 
# # D3:	Afghanistan, Angola, Bangladesh, Benin, Bhutan, Burkina Faso, Burundi, Cambodia, Cape Verde, Central African Republic, Chad, Comoros, Congo (Brazzaville), Congo (Kinshasa), Djibouti, Equatorial Guinea, Eritrea, Ethiopia, Gambia, Guinea, Guinea-Bissau, Guyana, Kiribati, Laos, Lesotho, Liberia, Madagascar, Malawi, Maldives, Mali, Mauritania, Mozambique, Myanmar, Nepal, Niger, Rwanda, Samoa, Sao Tome and Principe, Senegal, Sierra Leone, Solomon Islands, Timor-Leste, Togo, Tuvalu, Uganda, Tanzania, Vanuatu, Yemen, Zambia
# # 
# # EU:	Austria, Belgium, Cyprus, Czech Republic, Denmark, Finland, France, Germany, Greece, Hungary, Ireland, Italy, Luxembourg, Malta, Netherlands, Poland, Portugal, Slovakia, Slovenia, Spain, Sweden, United Kingdom
# # 
# # FSU:	Armenia, Azerbaijan, Belarus, Estonia, Georgia, Kazakhstan, Kyrgyzstan, Latvia, Lithuania, Moldova (Republic of), Russian Federation, Tajikistan, Turkmenistan, Ukraine, USSR (Former), Uzbekistan
# #
# ## summarize population growth by decade
# d50<-which(cities$Period %in% c("1950-1955", "1955-1960"))
# d60<-which(cities$Period %in% c("1960-1965", "1965-1970"))
# d70<-which(cities$Period %in% c("1970-1975", "1975-1980"))
# d80<-which(cities$Period %in% c("1980-1985", "1985-1990"))
# d90<-which(cities$Period %in% c("1990-1995", "1995-2000"))
# d00<-which(cities$Period %in% c("2000-2005", "2005-2010"))
# d10<-which(cities$Period %in% c("2010-2015", "2015-2020"))
# d20<-which(cities$Period %in% c("2020-2025"))
# cities$decade<-NA
# cities$decade[d50]<-"Fifties"
# cities$decade[d60]<-"Sixties"
# cities$decade[d70]<-"Seventies"
# cities$decade[d80]<-"Eighties"
# cities$decade[d90]<-"Nineties"
# cities$decade[d00]<-"Oughts"
# cities$decade[d10]<-"Twenty-Teens"
# cities$decade[d20]<-"Twenty-Twenties"
# 
# # summarize population growth by epoch
# e1<-which(cities$Period %in% c("1950-1955", "1955-1960", "1960-1965", "1965-1970", "1970-1975"))
# e2<-which(cities$Period %in% c("1975-1980", "1980-1985", "1985-1990", "1990-1995", "1995-2000"))
# e3<-which(cities$Period %in% c("2000-2005", "2005-2010", "2010-2015", "2015-2020", "2020-2025"))
# cities$epoch<-NA
# cities$epoch[e1]<-"1950-1975"
# cities$epoch[e2]<-"1975-2000"
# cities$epoch[e3]<-"2000-2025"
# 
# # save final data.frame
# save(cities, file="cities.rsav")
#
# epochal<-ddply(cities, .(Country.Code, Country, City.Code, Urban.Agglomeration, Latitude, Longitude, epoch), summarize, End.Pop.=Pop.[5], Annual.Growth=(Pop.[5]-Pop.[1])/Pop.[1]*100/25, Period.Growth= (Pop.[5]-Pop.[1])/Pop.[1]*100)
# check(epochal) # check for NAs and complete cases
# epochal<-na.omit(epochal)
# epochal$epoch<-as.factor(epochal$epoch)
# epochal$Period.Growth.Bin<-cut(epochal$Period.Growth, breaks=quantile(epochal$Period.Growth), include.lowest=TRUE)
# epochal$Period.Growth.Quartile<-factor(epochal$Period.Growth.Bin, labels=c("1st quartile","2nd quartile","3rd quartile","4th quartile"))
# check(epochal) # check for NAs and complete cases
# save(epochal, file="epochal.rsav")
```

```{r urbanization_map, fig.width=12, fig.height=6, echo=FALSE}
# base map
world <- map_data("world")
worldmap <- ggplot(world, aes(x=long, y=lat, group=group)) +
  geom_path()

# CURRENT Population and Urbanization Rates
ggplot() + 
  geom_path(data=world, alpha=0.5, size=0.2, aes(x=long, y=lat, group=group)) +
#   geom_ribbon(data=world, x=world$long, ymin=-23.45, ymax= 23.45, alpha=0.3, fill="green") +
#   geom_ribbon(data=world, x=world$long, ymin=-38, ymax=-23.45, alpha=0.3, fill="blue") +
#   geom_ribbon(data=world, x=world$long, ymin= 23.45, ymax= 38, alpha=0.3, fill="blue") +
  geom_hline(y=-23.45, linetype=2, size=0.2) +
  geom_hline(y= 23.45, linetype=2, size=0.2) +
  geom_hline(y=-38, linetype=3, size=0.2) +
  geom_hline(y=38, linetype=3, size=0.2) +
  geom_point(data=subset(cities, Period=="2010-2015"), aes(x=Longitude, y=Latitude, colour=Growth.Quartile, size=Pop./10^3)) +
#   coord_equal() +
  scale_y_continuous(breaks=(-2:2) * 30) +
  scale_x_continuous(breaks=(-4:4) * 45) +
  labs(colour = 'Urbanization Rate', size="Population (Millions)", title="Rapid Urbanization Throughout the Tropics and Sub-Tropics") +
  theme_bw() +
  scale_color_brewer(palette="Reds") +
  scale_size(range = c(1, 10))
  
# Evolution over time (10-year)
# decadal<-ddply(cities, .(Continent, Country.Code, Country, City.Code, Urban.Agglomeration, Latitude, Longitude, decade), numcolwise(mean))
# 
# decadal$decade<-factor(decadal$decade, levels=c("Fifties", "Sixties", "Seventies", "Eighties", "Nineties", "Oughts", "Twenty-Teens", "Twenty-Twenties"))
# 
# mapWorld <- borders("world", colour="gray40", fill="gray40")
# 
# ggplot() + 
#   geom_path(data=world, aes(x=long, y=lat, group=group)) +
#   mapWorld +
#   geom_point(data=decadal, aes(x=Longitude, y=Latitude, colour=Growth, size=Pop./10^3)) + 
#   facet_wrap(~decade, nrow=length(levels(factor(decadal$decade)))/2) +
#   coord_equal() +
#   scale_y_continuous(breaks=(-2:2) * 30) +
#   scale_x_continuous(breaks=(-4:4) * 45) +
#   labs(colour = 'Annual Population Growth (%)', size="Population (Millions)") +
#   theme_bw() +
#   scale_size(range = quantile(decadal$Pop., probs=c(0.0, 1)/500)) +
#   scale_colour_gradient(low="white", high="red", limits=quantile(decadal$Growth, probs=c(0.0, 1)))
# #   scale_size(range = quantile(decadal$Pop., probs=c(0.05, 0.95))/500) +
# #   scale_colour_gradient(low="white", high="red", limits=quantile(decadal$Growth, probs=c(0.05, 1)))

# Evolution over time (25 year)
ggplot() + 
  geom_path(data=world, aes(x=long, y=lat, group=group)) +
  geom_point(data=epochal, aes(x=Longitude, y=Latitude, colour=Period.Growth.Quartile, size=End.Pop./10^3)) + 
  facet_wrap(~epoch, nrow=length(levels(epochal$epoch))) +
  facet_wrap(~epoch) + 
  coord_equal() +
  scale_y_continuous(breaks=(-2:2) * 30) +
  scale_x_continuous(breaks=(-4:4) * 45) +
  labs(colour = 'Urbanization Rate', size="Population (Millions)") +
  theme_bw() +
  scale_color_brewer(palette="Reds") +
  scale_size(range = c(2, 10))

## Summary table of top 20 largest cities by population
test<-subset(cities, Period=="2010-2015")
test<-test[order(-test$Pop.), ]
test$Current.Pop.<-test$Pop./10^3

# give shorter names for print layout
usa<-which(test$Country=="United States of America")
test$Country[usa]<-"U.S.A"
uae<-which(test$Country=="United Arab Emirates")
test$Country[uae]<-"U.A.E"
DRC<-which(test$Country=="Democratic Republic of the Congo")
test$Country[DRC]<-"D.R.C"

megacities<-subset(test, Current.Pop.>=10, select=c("Continent","Country","Urban.Agglomeration","Current.Pop."))
rownames(megacities)<-NULL
colnames(megacities)[4]<-"Population [MM]"
print("World's Largest Cities in 2015 (Population > 10 million)")
print(megacities, digits=3)

fastcities<-test[order(-test$Growth), ]
fastcities<-subset(fastcities, select=c("Continent","Country","Urban.Agglomeration","Growth"))
rownames(fastcities)<-NULL
colnames(fastcities)[4]<-"Growth.Rate [%]"
print("World's Fastest Growing Cities (2010-2015) with a Population > 750,000")
print(fastcities[1:20,], digits=3)

remove(test)
```

```{r urbanization_PDF, echo=FALSE, fig.width=7, fig.height=4}
## density plots
test<-subset(cities, Year==2015)
sm.density.compare(test$Growth, test$region, xlab="% Change (Annual)", xlim=quantile(test$Growth, probs=c(0.0,.99)))
title(main="Population Growth Rate of Cities, by Region")
colfill<-c(2:(2+length(levels(test$region)))) 
legend("topright", levels(test$region), fill=colfill)

## also show cooling degree days for cities in these regions..
```

```{r urban_temps, eval=FALSE, cache=TRUE, fig.width=7, fig.height=4, fig.align='left'}
## Load list of cities from Hadley Urban Analysis
# file<-"http://www.metoffice.gov.uk/hadobs/urban/data/Station_list1.txt"
# stns<-read.fwf(file, widths=c(5,18,7,7), header = FALSE, sep = "\t", skip = 5, strip.white=TRUE)
# names(stns)<-c("WMONo", "Stn.name","Lat","Long")
# 
# ## load Hadley ISH station metadata
# load("/Users/elliotcohen/Dropbox/data/Climate/Hadley/metdata.rsav")
# metdata$USAFID<-as.factor(metdata$USAFID)
# 
# ## add region attribute to cities
# metdata$region<-NA
# t<-which(metdata$Lat >= -23.45 & metdata$Lat <= 23.45)
# metdata$region[t]<-"Tropics"
# 
# st<-which(metdata$Lat>= -38 & metdata$Lat < -23.45 | metdata$Lat > 23.45 & metdata$Lat <= 38)
# metdata$region[st]<-"Subtropics"
# 
# n<-which(metdata$Lat > 38)
# metdata$region[n]<-"North"
# 
# metdata$region<-as.factor(metdata$region)
# metdata<-subset(metdata, select=-WBAN)
# 
# ## load Hadley ISH raw data (only includes 1 zip file so far... add others!)
# load("/Users/elliotcohen/Dropbox/data/Electricity/CEA/Data/stnData.rsav")
# 
# ## remove erroneous values
# temps<-subset(stnData, select=-precip)
# temps<-subset(temps, temp > -100) # drop values below -100 deg. C
# temps<-subset(temps, temp < 200) # drop values above 200 deg. C (boiling)
# check(temps)
# temps$USAFID<-as.factor(temps$.id)
# 
# ## match metadat with observed data by USAFID
# df<-merge(temps, metdata, by="USAFID")
# 
# # ## match UN City data with Hadley ISD metadata...
# # ## use fuzzy logic...
# # get<-cities$City
# # grab<-metdata[which(metdata$City %in% get), ]
# 
# ## density plots
# sm.density.compare(df$temp, df$region, xlab="Temperature")
# title(main="Temperature Distribution by Region")
# colfill<-c(2:(2+length(levels(df$region)))) 
# legend("topleft", levels(df$region), fill=colfill)
```

Rationale
-----------------
Mid- to long-range global energy demand forecasts are typically reported as annual totals and provide little insight to the temporal distribution throughout the year. To address this shortcoming, we focus on the diurnal and seasonal distribution of energy demand and supply, which drive system cost but recieve relatively little attention in global energy outlooks. This requires more and better data than is typically available to researchers. 

A Global Survey
-------------------
This study presents an empirical analysis of diurnal-to-seasonal energy use patterns for XX global cities...

### Methods and Materials


### Downloading and processing NOAA weather station data in R

The National Oceanic and Atmospheric Administration (NOAA) offers a variety of free meteorological/climatological information through their data portal, the National Climatic Data Center.  The data is available on the [Online Climate Data Directory](http://www.ncdc.noaa.gov/cdo-web/) website, however it is also available via FTP, which is much more effecient, and is the method we'll be using here.

The root directory for each year's data is located here: [ftp://ftp.ncdc.noaa.gov/pub/data/noaa/](ftp://ftp.ncdc.noaa.gov/pub/data/noaa/).  

That is also where the list of weather stations is held. The current list of weather stations can be found in a file called **`isd-history.txt`** or **`isd-history.csv`**.  In the following example we will be using the .csv file. 

First, set the working directory you would like to download the weather station data to, then we'll load it into R. 
<br>

```{r NOAA: d/l weather stations, echo=TRUE}
require(ggmap)    # geocodes and map
require(FNN)      # knn function
require(gridExtra) 
require(plyr)

wd <- "~/Desktop/"
setwd(wd)

download.file(url = "ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.csv",
              destfile = "isd_weather_station_history.csv")
              
weather.stations <- read.csv("isd_weather_station_history.csv")
head(weather.stations)
```
<br>

For convenience, we'll change the names of two columns.

<br>
```{r NOAA: weather stations colnames, echo=TRUE}
colnames(weather.stations)[c(3, 9)] <- c("NAME", "ELEV")
```
<br>

We're going to be using the LAT and LON information to find the weather stations closest to our cities of interest, so we're going to eliminate the stations in this dataset that have NA values in either LAT or LON column.

The amount of data that is lost is minimal, approximately 1,300 observations out of 29,000.

<br>
```{r NOAA: remove NAs, echo=TRUE}
weather.stations <- weather.stations[!is.na(weather.stations$LAT) &
                                       !is.na(weather.stations$LON),]
```
<br> 

Now we're going create a list of cities that we want to find weather stations for.  Once we create the list, we'll use the **`geocodes()`** function in the **`ggmap`** package to get each city's LAT and LON coordiates. 

Lets use a few cities in the table of the World's Fastest Growing Cities mentioned above.

<br>
```{r NOAA: get cities, echo=TRUE, message=FALSE}
cities.of.interest <- c("Las Vegas, Nevada")
```
<br>

Now that we have the coordinates for our cities, we'll use the **`get.knnx()`** function in the **`FNN`** package to find the k-nearest weather stations to the coordinates in *`coordinates.of.interst`* dataset.

The following function, which uses **``get.knnx()``**, outputs a dataframe with the k-nearest weather stations to our list of cities, ranking each station (ie, 1st closest, 2nd closest, etc...), and extracting the year each station has started and ended recording data, which will be used in a subsequent function to actually download the data.  

This function has two input parameters:
1) *city*: this can be a single city, or a vector of cities
2) *k*: the number of stations to find

The output will be a dataframe called *`stations.of.interest`*. 

<br>
```{r NOAA: KNN, echo=TRUE, message=FALSE}
k.nearest.stations <- function(cities, k)
{
  coordinates.of.interest <- geocode(cities)
  nearest.stations <- get.knnx(as.matrix(weather.stations[,c(8,7)]),
                               as.matrix(coordinates.of.interest), k)
  
  stations.of.interest <- weather.stations[nearest.stations$nn.index[,],]
  
  stations.of.interest$rank <- rep(1:(nrow(stations.of.interest)/
                                     nrow(coordinates.of.interest)),
                                   each=nrow(coordinates.of.interest))
  
  stations.of.interest$city <- rep(cities,
                                   nrow(stations.of.interest)/
                                     nrow(coordinates.of.interest))
  
  stations.of.interest <- stations.of.interest[order(stations.of.interest$city),]
  
  stations.of.interest$BEGIN_Date <- as.numeric(substr(stations.of.interest$BEGIN,1,4))
  stations.of.interest$END_Date <- as.numeric(substr(stations.of.interest$END, 1, 4))
  
  return(stations.of.interest) 
}
nearest.stations <- k.nearest.stations(cities.of.interest, 7)

```
<br> 

Using the **``get_map()``** function we can check to see how close the weather stations are to our city's reference point by ploting the location of each weather station. 

In the following map, the black dot represents our reference point for the city, and the red dots represent the k-nearest weather stations. 

<br>
```{r NOAA: Plot map1, echo=TRUE, message=FALSE}
map <- get_map(location = cities.of.interest[1], zoom = 12)
````

```{r NOAA: Plot map2, echo=TRUE, message=FALSE}
ggmap(map) + 
  geom_point(aes(x = LON, y = LAT), 
             data = nearest.stations[nearest.stations$city==cities.of.interest[1],], 
             colour="red", size=7, alpha=.5) +
  geom_text(aes(x = LON, y = LAT, label=rank), 
             data = nearest.stations[nearest.stations$city==cities.of.interest[1],]) +
  geom_point(aes(x = lon, y = lat), 
             data = geocode(cities.of.interest[1]), 
             colour="black", size=7, alpha=.5)
```
<br>

Now let's downlaod the data.  The following function downloads the data for each weather station between a given data range.  

<br>
```{r NOAA: Station data, echo=TRUE, message=FALSE, eval=FALSE}
beg.date <- 2009
end.date <- 2009
get.weather.data <- function(stations, beg.date, end.date)
  {
  st.list <- stations[stations$BEGIN_Date <= beg.date &
                                    stations$END_Date >= end.date, ]
  temp.log <- as.data.frame(matrix(NA, nrow(st.list),4))
  colnames(temp.log) <- c("File","DL Status", "City", "rank")
  temp.log$City <- st.list$city
  temp.log$rank <- st.list$rank
  output.log <- data.frame()
  
  for (year in beg.date:end.date)
  {
    for (i in 1:nrow(temp.log))
    {
      temp.log[i, 1] <- paste(sprintf("%06d", st.list[i,1]), "-", 
                              sprintf("%05d", st.list[i,2]), "-", year,
                              ".gz", sep = "")
     
      tryCatch(
        {
          download.file(url=paste("ftp://ftp.ncdc.noaa.gov/pub/data/noaa/", year, "/", 
                      temp.log[i, 1], sep=""),
                      destfile = paste(temp.log[i,1]))
          
          temp.log[i,2] <- "Success"
        },
      
        error=function(cond){
          return(NA)
          next
        },
        
      finally={
          if(is.na(temp.log[i,2])=="TRUE") temp.log[i,2] <- "Failed"
      })
    } # end i

      output.log <- rbind(output.log, temp.log) # Combine each year's log
      output.log <- output.log[order(output.log$File),]
      #assign("output.log", output.log, envir = .GlobalEnv) 
      
  } # end year 

write.csv(output.log, paste("output_log", beg.date, end.date,
                            format(Sys.time(), "%s"), sep="_"), row.names=F)
return(output.log)
}
weather.data <- get.weather.data(nearest.stations, beg.date, end.date)
```
<br>

The files were downloaded in a .gz format, which need to be converted into .csv.  The following function converts each file into .csv and deletes the old .gz file.

The input will the name of the dataframe used in the previous code chunk.

<br>
```{r NOAA: File conversion, echo=TRUE, message=FALSE, eval=FALSE}
file.conversions <- function(downloaded.weather.data)
  {
  files <- list.files(wd, pattern="*.gz")
  column.widths <- c(4, 6, 5, 4, 2, 2, 2, 2, 1, 6,
                     7, 5, 5, 5, 4, 3, 1, 1, 4, 1,
                     5, 1, 1, 1, 6, 1, 1, 1, 5, 1,
                     5, 1, 5, 1)
  
  stations <- as.data.frame(matrix(NA, length(files),6))
  colnames(stations) <- c("USAFID", "WBAN", "YR", "LAT", "LONG", "ELEV")
  
  for (i in 1:length(files)) 
  {
    tryCatch({
      data <- read.fwf(gzfile(files[i]), column.widths)
      data <- data[, c(2:8, 10:11, 13, 16, 19, 29, 31, 33)]
      names(data) <- c("USAFID", "WBAN", "YR", "M",
                       "D", "HR", "MIN", "LAT", "LONG", "ELEV",
                       "WIND.DIR", "WIND.SPD", "TEMP", "DEW.POINT",
                       "ATM.PRES")
    },
    
    error=function(cond)
    {
      return(NULL)
      next
    })
    
    data$LAT <- data$LAT/1000
    data$LONG <- data$LONG/1000
    data$WIND.SPD <- data$WIND.SPD/10
    data$TEMP <- data$TEMP/10
    data$DEW.POINT <- data$DEW.POINT/10
    data$ATM.PRES <- data$ATM.PRES/10
    
    city.name <- unlist(lapply(strsplit(downloaded.weather.data[i,3],", "), function(x) x[1]))
    data$city <- city.name
    
    data$rank <- downloaded.weather.data[i,4]
    
    write.csv(data, file= paste(gsub(" ","_",city.name), "_",
                                 gsub("-","_",substr(downloaded.weather.data[i,1],1,17)),
                                 ".csv", sep = ""), row.names = FALSE)  
  }
  file.remove(files)
}
file.conversions(weather.data)
```
<br> 

The final step here is to combine all the files from the same weather station together and save it into one .csv so that we can assess which station has the best data for our purposes.

<br>
```{r NOAA: Combine csvs, echo=TRUE, message=FALSE, eval=FALSE}
files <- list.files(wd,pattern="[0-9].csv$")
for (i in 1:nrow(count(substr(files, 1, nchar(files)-9))))
  {
  counts <- as.vector(count(substr(files, 1, nchar(files)-9))[,2])
  beg.count <- sum(counts[1:(i-1)])+1
  end.count <- sum(counts[1:i])
  
  if(i == 1) {start <- 1} else {start <- beg.count}
  
  station.data <- lapply(files[start:end.count],
                         read.csv, header = TRUE)
  
  combined.df <- do.call(rbind , station.data)
  
  write.csv(combined.df, 
            paste(as.character(count(substr(files, 1, nchar(files)-9))[i,1]),".csv",sep=""))
  rm(combined.df)
  file.remove(files[start:end.count])
}
```

Now lets read the combined files into one list:

```{r NOAA: read files into a list, eval=FALSE}
load.files <- list.files(wd,pattern="[0-9].csv$")
csv.list <- lapply(load.files, read.csv)
city.names <- substr(load.files, 1, nchar(load.files)-17)
station.num <- substr(gsub("[[:punct:]]","",gsub("[[:alpha:]]+","", load.files)),1,6)
names(csv.list) <- paste(city.names, station.num, sep="_")

lapply(csv.list, function(x) dim(x)[1])
head(csv.list[[1]])


# eliminate stations that don't have enough data
filter.stations <- function(list.of.stations, beg, end)
  {
  
  # These parameters can be built into the function as inputs if desired:
  num.of.years <- end-beg+1
  days.in.year <- 365
  min.interval <- 3  # minimum acceptable hourly intervals
  hrs.in.day <- 24
  tolerance <- .95 # allow for 5% of the data to be missing
  min.acceptable.obs <- (hrs.in.day/min.interval)*days.in.year*num.of.years*tolerance
  
  # Stations that don't meet requirements and will be removed
  removed <- names(list.of.stations[which(sapply(list.of.stations, nrow) < min.acceptable.obs)]) 
  
  # Stations that meet the requirements:
  kept <- names(list.of.stations[-which(sapply(list.of.stations, nrow) < min.acceptable.obs)])
  
  # Createing DFs to track what is removed/kept
  stations.df <- data.frame(count(city.names))
  removed.df <- count(substr(removed, 1, nchar(removed)-7))
  kept.df <- count(substr(kept, 1, nchar(kept)-7))
  
  # Put the dataframes into a list and merge them all at once:
  df.list <- list(stations.df, removed.df, kept.df)
  merged.df <- Reduce(function(...) merge(..., by="x", all=T), df.list)
  merged.df[is.na(merged.df)] <- 0
  colnames(merged.df) <- c("city", "stations", "removed", "kept")
  
  # print the results:
  print(merged.df)
  
  # Return a list of dataframes that will be kept (ie, they meet the requirements set above)
  return(list.of.stations[-which(sapply(list.of.stations, nrow) < min.acceptable.obs)])
  
  }
new.list <- filter.stations(csv.list, beg.date, end.date)

new.list <- csv.list

cities.left <- substr(kept, 1, nchar(kept)-7)
i <- 1
for (i in 1:length(cities.left))
  {
   if (length(cities.left[i])==1) next else 
  }


lapply(new.list, function(x) dim(x)[1])


# Check to see that TEMP and DEW.POINT have 999.9s
for(i in 1:length(new.list)){print(max(new.list[[i]]["TEMP"]))}
for(i in 1:length(new.list)){print(max(new.list[[i]]["DEW.POINT"]))}
for(i in 1:length(new.list)){print(max(new.list[[i]]["WIND.DIR"]))}
for(i in 1:length(new.list)){print(max(new.list[[i]]["WIND.SPD"]))}


# Transforming all the 999s. 
# Loop through the list, each column of interest in each dataframe
# Change the observations from 999 to the prior observation
columns <- c("TEMP", "DEW.POINT")
index <- NULL
index.count <- as.data.frame(matrix(nrow=length(new.list), ncol=length(columns)))
colnames(index.count) <- columns
rownames(index.count) <- names(new.list)
for (L in 1:length(new.list))
{
  for (i in 1:length(columns))
  {
    index <- which(new.list[[L]][columns[i]]==999.9)
    index.count[L,i] <- length(index)
    for (j in 1:length(index))
    {
      new.list[[L]][,columns[i]][index][j] <- new.list[[L]][,columns[i]][index-1][j]
    }
  }
}
print(index.count)

# Since we're interested in hourly observations, we'll ddply over each
# dataframe and average all the observations for each hour.
clean.list <- lapply(new.list, function(x){
              ddply(x, .(city, USAFID, rank, YR, M, D, HR), summarise ,LAT=mean(LAT),
                    LONG=mean(LONG), ELEV=mean(ELEV),
                    WIND.DIR=mean(WIND.DIR), WIND.SPD=mean(WIND.SPD),
                    TEMP=mean(TEMP), DEW.POINT=mean(DEW.POINT))})

lapply(clean.list, function(x) dim(x)[1])
head(clean.list[[3]])
test <- clean.list[[3]]
test.2 <- test[test$HR == 0,]
write.csv(clean.list[[3]], "Las_Vegas_Hourly_Temp_2009.csv", row.names=F)

```



### Temperature Load Relationship

```{r, echo=FALSE}
setwd("~/Google Drive/Global_Trends/Data")
require(segmented)
require(segmented)
require(ggplot2)
require(reshape)
require(plyr)
require(lubridate)
```

This section studies the relationship between load and temperature for 35 cities in North America, West Africa,  South and South East Asia. 

The full list includes: Berlin, Bishop, Bowman, Bridgeville, Calumet, Chandigarh, Dayton, Delhi, Dighton,          El Paso, Ennis, Greenville, Ishpeming, Kingsbury, Las Vegas, Lincoln, Los Angeles, Maunaloa, Mindelo,           NYC, Pelham, Philadelphia, Portales, Prince Frederick, Remer, Sacramento, San Diego, Sebring, Sidney Center, Singapore, South Elgin, Tipton, Union Springs, Vashon, Winona. 

Starting from high definition load and hourly data (i.e. at least 2 observations per day), the daily minimum and maximum temperature and load are derived for all the cities. 

The below code shows the data manipulation for Mindelo, Capre Verde. 

```{r, echo=TRUE}

################################### for Sao Vicente

# Loading the data
mindelo.load <- read.csv("SaoVicente_hourly_load.csv", header = TRUE, sep = "," , row.names = NULL)
mindelo.temp <- read.csv("Mindelo-SaoVicente-CapeVerde-weather.csv", header = TRUE, sep = "," , row.names = NULL) 

# Temperature
# Organizing the data
mindelo.temp$city <- "mindelo"
mindelo.temp <- mindelo.temp[,c(17,14,4:8)]
# COrrecting for erroneaous data
index <- which(mindelo.temp$TEMP==999.9)
for (j in 1:length(index))
{
  mindelo.temp$TEMP[index][j] <- mindelo.temp$TEMP[index-1][j]
}
mindelo.temp$date <- as.POSIXct(paste(as.character(mindelo.temp$M),as.character(mindelo.temp$D),as.character(mindelo.temp$YR), sep="-"),format="%m-%d-%Y",tz="GMT")
# Deriving the daily min, mean and max temperature
mindelo.temp <- ddply(mindelo.temp, .(date,city), summarize, min_T=min(TEMP), mean_T=mean(TEMP), max_T=max(TEMP))

# Load
# Organizing the data
mindelo.load$date.time <- as.POSIXct(paste(as.character(mindelo.load$date.time)), format="%m/%d/%y %H:%M", tz="US/Pacific")
mindelo.load$date <- as.Date(mindelo.load$date.time)
mindelo.load$city <- "mindelo"
mindelo.load$demand <- as.numeric(gsub(",", "", as.character(mindelo.load$demand)))
# Deriving the daily min, mean and max load
mindelo.load <- ddply(mindelo.load, .(date, city), summarize, min_MW=min(demand),mean_MW=mean(demand), max_MW=max(demand))

# Merging
mindelo <- merge(mindelo.load, mindelo.temp, by=c("date", "city"))
```

The same procedure is repeated for all the cities cites above. 

```{r, echo=FALSE}
################################### For US cities

# For Demand

# Set WD
setwd("~/Google Drive/Global_Trends/Data/US_Hourly_Load/Combined_Demand_Data/")
# Load Demand data
load <- read.csv("US_cities_load.csv")
cities <- levels(load$city)
# Arrange the format
load$time <- as.POSIXct(paste(as.character(load$time)), format="%Y-%m-%d %H:%M:%S", tz="GMT")
load$Load <- as.numeric(as.character(load$Load))
load$Load[which(is.na(load))] <- load$Load[which(is.na(load))-1]
load$Date <- as.Date(load$time)
load <- load[,c(7,8,1)]
# Get the daily demand by city
d <- ddply(load, .(city, Date), summarize, min_MW=min(Load), mean_MW=mean(Load), max_MW=max(Load))



# For Temperature

setwd("~/Google Drive/Global_Trends/Data/US_Weather_Station_Data/")

files <- list.files(getwd(),pattern=".csv")

for (i in 1:length(files))
{
  temp.df <- read.csv(files[i], stringsAsFactors=F)
  temp.df <- temp.df[,c(16,13,3:7)]
  temp.df$date <- as.POSIXct(paste(as.character(temp.df$M),as.character(temp.df$D),as.character(temp.df$YR), sep="-"),format="%m-%d-%Y",tz="GMT")
  temp.df$datetime<-as.POSIXct(paste(paste(as.character(temp.df$date),as.character(temp.df$HR),sep="   "),":00:00",sep=""),tz="GMT")
  # Correct for erroneous temperature measurement
  index <- which(temp.df$TEMP==999.9)
  for (j in 1:length(index))
  {
    temp.df$TEMP[index][j] <- temp.df$TEMP[index-1][j]
  }
  temp.df <- ddply(temp.df, .(date,city), summarize, min_T=min(TEMP), mean_T=mean(TEMP), max_T=max(TEMP))
  #if (nrow(temp.df)!=365) {
  #  temp.df <- NULL
  #} else {
  assign(substr(files[i], 1, nchar(files[i])-4), temp.df)  
  #}
  
}

t <- rbind(`Berlin-1`, `Bishop-1`, `Bowman-1`, `Bridgeville-1`, `Calumet-2`, `Dayton-1`, `Dighton-1`, `El Paso-1`, `Ennis-2`, `Greenville-1`, `Ishpeming-2`, `Kingsbury-1`,`Las Vegas-1`, `Lincoln-1`, `Los Angeles-1`, `Maunaloa-1`, `Pelham-1`, `Portales-2`, `Prince Frederick-2`, `Remer-1`, `Sacramento-1`, `San Diego-1`, `Sebring-2`, `Sidney Center-2`, `South Elgin-1`, `Tipton-2`, `Union Springs-1`, `Vashon-1`, `Winona-1` )
colnames(t)[1] <- "Date"
t$city <- as.factor(t$city)
t <- t[,c(2,1,3:5)]
t$Date <- as.Date(t$Date)

# Merge daily Temperature and Demand for all cities
uscities <- merge(t,d,by=c("Date", "city"))


################################### For Singapore, Delhi and Chandigarh

# Loading the required data

setwd("~/Google Drive/Global_Trends/Data")

# For Delhi
delhi.load <- read.csv("Delhi_load_hourly.csv", header=TRUE)
delhi.temp <- read.csv("Delhi_temperature_daily.csv", header=TRUE)

# For Chandigarh
chandigarh.load <- read.csv("Chandigarh_load_hourly.csv", header=TRUE)
chandigarh.temp <- read.csv("Chandigarh_temperature_daily.csv", header=TRUE)

#For Singapore
singapore.load <- read.csv("Singapore_demand_data.csv", header = TRUE, sep = ",", row.names = NULL)
singapore.temp <- read.csv("Singapore_weather_data.csv", header = TRUE, sep = ",", row.names = NULL)

# Rearranging data in the convenient format

# For Delhi
delhi.load$date.time <- as.POSIXct(paste(as.character(delhi.load$date.time)),format="%Y-%m-%d %H:%M:%S", tz="US/Pacific")
delhi.load$date <- as.Date(delhi.load$date.time)

# For Chandigarh
chandigarh.temp$date <- as.Date(chandigarh.temp$date)
chandigarh.load$date.time <- as.POSIXct(paste(as.character(chandigarh.load$date.time)),format="%Y-%m-%d %H:%M:%S", tz="US/Pacific")
chandigarh.load$date <- as.Date(chandigarh.load$date.time)

# For Singapore
singapore.temp <- singapore.temp[,c(4:8,14)]
singapore.temp$date <- as.POSIXct(paste(as.character(singapore.temp$M),as.character(singapore.temp$D),as.character(singapore.temp$YR), sep="-"),format="%m-%d-%Y",tz="US/Pacific")
singapore.temp$city <- "singapore"
# singapore.load$Date <- as.Date(singapore.load$Date)
singapore.load$city <- "singapore"
singapore.load$date <- as.POSIXct(paste(as.character(singapore.load$Date)),format="%Y-%m-%d", tz="US/Pacific")

# Merging load and temperature for all cities

# For Delhi
d<-ddply(delhi.load, .(city, date), summarize, min_MW=min(demand), mean_MW=mean(demand), max_MW=max(demand))
t<-delhi.temp #ddply(delhi.temp, .(Date), summarize, min_T=min(TEMP), mean_T=mean(TEMP), max_T=max(TEMP))
delhi<-merge(d,t,by=c("date", "city"))

# For Chandigarh
d<-ddply(chandigarh.load,.(city, date), summarize, min_MW=min(demand), mean_MW=mean(demand), max_MW=max(demand))
t<-chandigarh.temp #ddply(chandigarh.temp,.(Date), summarize, min_T=min(TEMP), mean_T=mean(TEMP), max_T=max(TEMP))
chandigarh<-merge(d,t,by=c("date","city"))

# For Singapore
d <- ddply(singapore.load, .(date,city), summarize, min_MW=min(Demand), mean_MW=mean(Demand), max_MW=max(Demand))
t <- ddply(singapore.temp, .(date,city), summarize, min_T=min(TEMP), mean_T=mean(TEMP), max_T=max(TEMP))
singapore <- merge(d,t,by=c("date","city"))

################################### For NYC

# Load hourly temperatures
temps <- read.csv("NYC_weather_data.csv", header = TRUE, sep = ",", row.names = NULL)

# melt data into big df
test<-melt(temps, na.rm=TRUE)
test$POSIXct<-seq(as.POSIXct("2006-01-01 00:00:00", format='%Y-%m-%d %H:%M:%S'), as.POSIXct("2012-12-31 23:00:00", format='%Y-%m-%d %H:%M:%S'), by="hour")
# drop the year
test$City<-"NYC"
test$Date<-as.Date(test$POSIXct)
test2<-test[,c(4,3,5,2)] # grab City, POSIXct, Date, value
names(test2)[4]<-"temp" 
NYCtemps<-test2
NYCtemps$temp<-NYCtemps$temp + 273

# Load hourly electricity demand. 
demand <- read.csv("NYC_demand_data.csv", header = TRUE, sep = ",", row.names = NULL)

# melt data into big df....
test<-melt(demand, na.rm=TRUE)
test$POSIXct<-seq(as.POSIXct("2006-01-01 00:00:00", format='%Y-%m-%d %H:%M:%S'), as.POSIXct("2012-12-31 23:00:00", format='%Y-%m-%d %H:%M:%S'), by="hour") # drop the year
test$City<-"NYC"
test$Date<-as.Date(test$POSIXct)
test2<-test[,c(4,3,5,2)] # grab City, POSIXct, Date, value
names(test2)[4]<-"demand" 
NYCdemand<-test2

# merge demand and temperature data
colnames(NYCdemand)[c(1,3)] <- c("city", "date")
colnames(NYCtemps)[c(1,3)] <- c("city", "date")
d<-ddply(NYCdemand,.(city, date), summarize, min_MW=min(demand), mean_MW=mean(demand), max_MW=max(demand))
t<-ddply(NYCtemps,.(city, date), summarize, min_T=min(temp), mean_T=mean(temp), max_T=max(temp))
nyc<-merge(d, t, by=c("city","date"))


################################### For Philadelhia

# For Temperature

# Loading Philadelphia Temp data
philly_temp <- read.csv("~/Google Drive/Global_Trends/Data/Philly_Weather_Data/Philly-724080-13739-2011.csv")
# Extracting the columns of interets
philly_temp<-philly_temp[,c(3:7,13)]
# Getting rid of unwanted data: closest approximation is the previous value
philly_temp$TEMP[which(philly_temp$TEMP>100)]<-philly_temp$TEMP[which(philly_temp$TEMP>100)-1]
philly_temp$TEMP[which(philly_temp$TEMP>100)]<-philly_temp$TEMP[which(philly_temp$TEMP>100)-1]
# Deriving hourly mean, max and min Temp Data
philly_temp<-ddply(philly_temp, .(YR, M, D), summarize, min_T=min(TEMP), mean_T=mean(TEMP), max_T=max(TEMP))
# Creating a date stamp
philly_temp$date <- as.POSIXct(paste(as.character(philly_temp$M),as.character(philly_temp$D),as.character(philly_temp$YR), sep="-"),format="%m-%d-%Y", tz="US/Pacific")
# Organizing the data frame
philly_temp$city<-"philadelphia"
philly_temp<-philly_temp[,c(8,7,4:6)]

# For Load

# Loading Load data for Philadelphia
philly_load <- read.csv("Philly_demand_data.csv")
# Isolating data of interest
philly_load<-philly_load[,c(1,5)]
# Creating date stamp
philly_load$Date <- as.Date(as.character(philly_load$Date), "%m/%d/%y")
# Organizing columns
colnames(philly_load)[1]<-"date"
colnames(philly_load)[2]<-"demand"
philly_load$city <- "philadelphia"
philly_load <- philly_load[,c(3,1,2)]
philly_load <- ddply(philly_load, .(city, date), summarise, min_MW=min(demand), mean_MW=mean(demand), max_MW=max(demand))

# Merging Load and Temp data by datetime and hour
philadelhia<-merge(philly_load,philly_temp,by=c("date", "city"))


### Binding all data frames into one big data frame
colnames(uscities)[1] <- "date"
df <- rbind(singapore, delhi, chandigarh, uscities, mindelo, nyc, philadelhia)

### Adding seasons to the dataframe

# Setting seasons for plotting
df$month <- month(df$date)

for (i in 1:nrow(df))
  {
  if (df$month[i]==3 |df$month[i]==4 |df$month[i]==5) df$season[i] <- "spring"
  else if (df$month[i]==6 |df$month[i]==7 |df$month[i]==8) df$season[i] <- "summer"
  else if (df$month[i]==9 |df$month[i]==10 |df$month[i]==11) df$season[i] <- "fall"
  else if (df$month[i]==12 |df$month[i]==1 |df$month[i]==2) df$season[i] <- "winter"
  }
```

Once the daily minimum and maximum temperature and load have been derived for all the cities, a segmented linear regression model is fitted for the minimum and maximum pairs of temperature and load for every city. This is done as follow. 

```{r, echo=TRUE}
### Fitting linear segmented model
df$city <- as.factor(df$city)
cities <- levels(df$city)

prop <- data.frame(cities)
prop$min_intercept <- 0
prop$max_intercept <- 0
prop$min_gradient1 <- 0
prop$max_gradient1 <- 0
prop$min_gradient2 <- 0
prop$max_gradient2 <- 0

a <- b <- NULL

for (i in 1:length(cities))
{
  a[i] <- mean(df$min_T[which(df$city==cities[i])])
  b[i] <- mean(df$max_T[which(df$city==cities[i])])
}

for (i in 1:length(cities))
{
  lm.min <- lm(min_MW~min_T,data=df[which(df$city==cities[i]),])
  seg.min <- segmented(lm.min, seg.Z=~min_T, psi=list(min_T=a[i]), control=seg.control(display=FALSE))
  lm.max <- lm(max_MW~max_T,data=df[which(df$city==cities[i]),])
  seg.max <- segmented(lm.max, seg.Z=~max_T, psi=list(max_T=b[i]), control=seg.control(display=FALSE))
  prop$min_intercept[i] <- coef(seg.min)[1]
  prop$max_intercept[i] <- coef(seg.max)[1]
  prop$min_gradient1[i] <- coef(seg.min)[2]
  prop$max_gradient1[i] <- coef(seg.max)[2]
  prop$min_gradient2[i] <- coef(seg.min)[3]
  prop$max_gradient2[i] <- coef(seg.max)[3]
}
colnames(prop)[4:7] <- c("min_heating_gradient", "max_heating_gradient", "min_cooling_gradient", "max_cooling_gradient")
```

#### Singapore, Delhi and Chandigarh

Isolating Singapore, Delhi and Chandigarh, the minimum daily load against the minimum daily temperature is plotted below: 


```{r, echo=FALSE}
ggplot() +
  geom_point(data=df[which(df$city==c("singapore", "delhi", "chandigarh")),], aes(x=min_T, y=min_MW  , color=season))+
  labs(x="Minimum Temperature (Deg C)",y="Minimum Demand (MW)", title="Temperature Load relationship")+
  theme(legend.position="bottom",legend.title=element_blank(),axis.title.x=element_text(),axis.title.y=element_text()) +
  facet_wrap(~city, scale="free")
```

In a similar way, the maximum daily load against the maximum daily temperature is plotted: 
```{r, echo=FALSE}
ggplot() +
  geom_point(data=df[which(df$city==c("singapore", "delhi", "chandigarh")),], aes(x=max_T, y=max_MW, color=season ))+
  labs(x="Maximum Temperature (Deg C)",y="Maximum Demand (MW)", title="Temperature Load relationship")+
  theme(legend.position="bottom",legend.title=element_blank(),axis.title.x=element_text(),axis.title.y=element_text()) +
  facet_wrap(~city, scale="free")
```

And finally the minumum and maximum relationships are plotted on the same graph: 
```{r, echo=FALSE}
ggplot() +
  geom_point(data=df[which(df$city==c("singapore", "delhi", "chandigarh")),], aes(x=min_T, y=min_MW  , color="Minimum"))+
  geom_point(data=df[which(df$city==c("singapore", "delhi", "chandigarh")),], aes(x=max_T, y=max_MW  , color="Maximum"))+
  labs(x="Temperature (Deg C)",y="Demand (MW)", title="Temperature Load relationship")+
  theme(legend.position="bottom",legend.title=element_blank(),axis.title.x=element_text(),axis.title.y=element_text()) +
  facet_wrap(~city, scale="free")
```

The table of intercepts and gradients is as follow: 
```{r, echo=FALSE}
prop[which(prop$cities %in% c("singapore","delhi","chandigarh")),]
```

#### New York City and Philadelphia

Isolating New York City and Philadelphia, the minimum daily load against the minimum daily temperature is plotted below: 

```{r, echo=FALSE}
ggplot() +
  geom_point(data=df[which(df$city==c("NYC", "philadelphia")),], aes(x=min_T, y=min_MW  , color=season))+
  labs(x="Minimum Temperature (Deg C)",y="Minimum Demand (MW)", title="Temperature Load relationship")+
  theme(legend.position="bottom",legend.title=element_blank(),axis.title.x=element_text(),axis.title.y=element_text()) +
  facet_wrap(~city, scale="free")
```

In a similar way, the maximum daily load against the maximum daily temperature is plotted: 
```{r, echo=FALSE}
ggplot() +
  geom_point(data=df[which(df$city==c("NYC", "philadelphia")),], aes(x=max_T, y=max_MW, color=season ))+
  labs(x="Maximum Temperature (Deg C)",y="Maximum Demand (MW)", title="Temperature Load relationship")+
  theme(legend.position="bottom",legend.title=element_blank(),axis.title.x=element_text(),axis.title.y=element_text()) +
  facet_wrap(~city, scale="free")
```

And finally the minumum and maximum relationships are plotted on the same graph: 
```{r, echo=FALSE}
ggplot() +
  geom_point(data=df[which(df$city==c("NYC", "philadelphia")),], aes(x=min_T, y=min_MW  , color="Minimum"))+
  geom_point(data=df[which(df$city==c("NYC", "philadelphia")),], aes(x=max_T, y=max_MW  , color="Maximum"))+
  labs(x="Temperature (Deg C)",y="Demand (MW)", title="Temperature Load relationship")+
  theme(legend.position="bottom",legend.title=element_blank(),axis.title.x=element_text(),axis.title.y=element_text()) +
  facet_wrap(~city, scale="free")
```

The table of intercepts and gradients is as follow: 
```{r, echo=FALSE}
prop[which(prop$cities %in% c("NYC","philadelphia")),]
```

#### The rest of US cities

Isolating the rest of US cities, the minimum daily load against the minimum daily temperature is plotted below: 

```{r, echo=FALSE}
ggplot() +
  geom_point(data=df[which(df$city!=c("NYC", "philadelphia","singapore", "chandigarh","delhi","mindelo")),], aes(x=min_T, y=min_MW  , color=season))+
  labs(x="Minimum Temperature (Deg C)",y="Minimum Demand (MW)", title="Temperature Load relationship")+
  theme(legend.position="bottom",legend.title=element_blank(),axis.title.x=element_text(),axis.title.y=element_text()) +
  facet_wrap(~city, scale="free")
```

In a similar way, the maximum daily load against the maximum daily temperature is plotted: 
```{r, echo=FALSE}
ggplot() +
  geom_point(data=df[which(df$city!=c("NYC", "philadelphia","singapore", "chandigarh","delhi","mindelo")),], aes(x=max_T, y=max_MW, color=season ))+
  labs(x="Maximum Temperature (Deg C)",y="Maximum Demand (MW)", title="Temperature Load relationship")+
  theme(legend.position="bottom",legend.title=element_blank(),axis.title.x=element_text(),axis.title.y=element_text()) +
  facet_wrap(~city, scale="free")
```

And finally the minumum and maximum relationships are plotted on the same graph: 
```{r, echo=FALSE}
ggplot() +
  geom_point(data=df[which(df$city!=c("NYC", "philadelphia","singapore", "chandigarh","delhi","mindelo")),], aes(x=min_T, y=min_MW  , color="Minimum"))+
  geom_point(data=df[which(df$city!=c("NYC", "philadelphia","singapore", "chandigarh","delhi","mindelo")),], aes(x=max_T, y=max_MW  , color="Maximum"))+
  labs(x="Temperature (Deg C)",y="Demand (MW)", title="Temperature Load relationship")+
  theme(legend.position="bottom",legend.title=element_blank(),axis.title.x=element_text(),axis.title.y=element_text()) +
  facet_wrap(~city, scale="free")
```

The table of intercepts and gradients is as follow: 
```{r, echo=FALSE}
prop[which(prop$cities %in% c("NYC","philadelphia")),]
```



#### Mindelo, Cape Verde

Isolating Mindelo the minimum daily load against the minimum daily temperature is plotted below: 

```{r, echo=FALSE}
ggplot() +
  geom_point(data=df[which(df$city=="mindelo"),], aes(x=min_T, y=min_MW  , color=season))+
  labs(x="Minimum Temperature (Deg C)",y="Minimum Demand (MW)", title="Temperature Load relationship")+
  theme(legend.position="bottom",legend.title=element_blank(),axis.title.x=element_text(),axis.title.y=element_text()) +
  facet_wrap(~city, scale="free")
```

In a similar way, the maximum daily load against the maximum daily temperature is plotted: 
```{r, echo=FALSE}
ggplot() +
  geom_point(data=df[which(df$city=="mindelo"),], aes(x=max_T, y=max_MW, color=season ))+
  labs(x="Maximum Temperature (Deg C)",y="Maximum Demand (MW)", title="Temperature Load relationship")+
  theme(legend.position="bottom",legend.title=element_blank(),axis.title.x=element_text(),axis.title.y=element_text()) +
  facet_wrap(~city, scale="free")
```

And finally the minumum and maximum relationships are plotted on the same graph: 
```{r, echo=FALSE}
ggplot() +
  geom_point(data=df[which(df$city=="mindelo"),], aes(x=min_T, y=min_MW  , color="Minimum"))+
  geom_point(data=df[which(df$city=="mindelo"),], aes(x=max_T, y=max_MW  , color="Maximum"))+
  labs(x="Temperature (Deg C)",y="Demand (MW)", title="Temperature Load relationship")+
  theme(legend.position="bottom",legend.title=element_blank(),axis.title.x=element_text(),axis.title.y=element_text()) +
  facet_wrap(~city, scale="free")
```

The table of intercepts and gradients is as follow: 
```{r, echo=FALSE}
prop[which(prop$cities %in% "mindelo"),]
```


Literature Review
-------------------

We collected and read literature on topics including energy usage, global energy trends, climate-electricty relation, statistical analysis methods, etc. It was a great help in problem clarification, research design processes.

The following two papers are found to be very referable.

### 1. Climatic-related Evaluations of the Summer Peak-hours’ electric load in Israel
* M. Segal; H. Shafri; M. Mandel; P. Alpert; Y. Balmor (Journal of Applied Meteorology, 1992)

This study evaluated the interrelationship between the summer peak electricity load and pertinent meteorological parameters. The author proposed in this paper a regression model refecting how climate parameters (temperature, humidity, wind spead) influence summer peak electricity load, which is widely accepted and well developed by successive researchers.


### 2. Modelling changes to electricity demand load duration curves as a consequence of predicted climate change for Australia
* Marcus J. Thatcher (Energy, 2007)

This research was developed on the base of Segal's research and took Segal's model to a completely different level. It brought up valid ways (sine-exponential model) using only daily max/min temperature and relative humidity to estimate apparent temperature in 30 mins intervals. Most importantly, the author designed and testified an effective electricity demand predicting model, which uses time (data and specific time) and relative temperatures as input and outputs estimated electriicty demand in 30 mins intervals.


### 3. Residential Consumption of Electricity in India Documentation of Data and Methodology
* The World Bank (2008)

##### Introduction
It’s a background paper for the study India: Strategies for Low Carbon Growth, describing the methodology and results of  the household electricity consumption model, which is to project the numbers of electricity appliances in use in the residential sector in India to fiscal 2031-32. The study is designed to conduct sensitivity analysis for different future scenarios, 2 of which are distributed in this paper.

 **Scenario 1**: models the absorption of the appliances currently on sale in India into the appliance population. 
 
 **Scenario 2**: with higher energy efficiency
Notes: Neither of the two takes the rebound effect into account.

##### Electricity Demand Projection Models
Many different approaches for electricity demand projection are found in literature, and some of them have focused on India’s demand especially. All these approaches are either from a macro-level or micro-level perspective.

 **Macroeconomic Approach** (For example, Bose and Shukla 1999, CEA 2007a). 

Use macro data at the country or sub-national/state level to estimate the income elasticity of electricity consumption by econometric analysis of the relationship between electricity consumption and its key determinants.

 **Microeconomic Approach** (For example Pachauri (2004), Filippini and Pachauri (2004), and Tiwari (2000))

Use micro-level data that reflects individual and household behavior and take into account a number of household characteristics to analyze across different heterogeneous household sub-groups.

##### Model: Forecasting  household electricity consumption.
The approach used in this paper is a variant of the microeconomic approach that is sometimes referred to as an end-use or bottom-up approach. It’s different from the microeconomic approach in a way that it examines the ownership and the use of household electricity-consuming devices and considers efficiency scenarios from an engineering point of view, instead of analyzing a income-demand relationship through a reduced-form equation. The advantage over other approaches is that it allows the assessment of efficiency scenarios for electrical appliances, their usage, and electricity conservation as well as the impact of other economic, demographic, and geographical factors.

The Household electricity sector module is a tool kit that allows any number of scenarios to be investigated and run.

* **Step 1**: project numbers of households and their expenditures in urban and rural areas. 
* **Step 2**: identify households that are electrified. 
* **Step 3**: forecast ownership of appliances among electrified households. 
* **Step 4**: computes electricity consumption. 


### 4. Penetrtion of Solar Power without Storage
* Nathan Stodola, Vijay Modi (Energy Policy, 2009)

Solar power cannot provide all of the electricity needs of our civilization without some significant investments in infrastructure or storage capabilities. This paper shows a way that enable photovoltaic panels to produce a significant share of US electricity needs without such huge investments, due to the synchronicity of power demand with sunshine. A measure of this alignment is computed from regional demand and solar intensity data for 32 regions in the United States, using 2005 solar and electric data. The assumption is that the output of the baseload plants at a given hour is equal to lowest demand within one week on either side of the hour. 

*baseload + Dispatchable + Solar = System Load*

The results show that the energy produced from solar panels would represent over 7% of the present total annual electrical load in the US. 

